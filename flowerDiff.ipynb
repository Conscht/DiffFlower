{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-w4V-jnHx_j"
   },
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22460,
     "status": "ok",
     "timestamp": 1768774406155,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "iYizbMTmHis4",
    "outputId": "5028b1a5-951b-4a4d-f34e-c89b73651a0c"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries (Commented out potentially conflicting versions)\n",
    "# !pip install fiftyone wandb open-clip-torch\n",
    "# !pip install ftfy regex tqdm torchmetrics torch-fidelity\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# NOTE: The following line tries to downgrade PyTorch to 1.7.1 which conflicts with our current setup.\n",
    "# !pip install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6ZanO1vH58z"
   },
   "source": [
    "## Part 1: Image Generation and Emebdding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1768774406219,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "8biQUK45iDUu",
    "outputId": "c3d6a698-10e3-464e-e13e-ebfa1104ad9e"
   },
   "outputs": [],
   "source": [
    "# cd drive/MyDrive/Hands-on-CV3  # Commented out Google Colab specific path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3009,
     "status": "ok",
     "timestamp": 1768774409231,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "vD8D59RoH-64"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import our new utility module\n",
    "import utils\n",
    "from utils import ClassicLeNet5, CustomTorchImageDataset, train_epoch, val_epoch, evaluate_idk_performance\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(\"Utils imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8915,
     "status": "ok",
     "timestamp": 1768774418178,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "y241swK7fkou"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qRaMxnfynLt"
   },
   "source": [
    "Sanity Check of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 12409,
     "status": "ok",
     "timestamp": 1768774430592,
     "user": {
      "displayName": "Red Lp",
      "userId": "12186890915998845396"
     },
     "user_tz": -60
    },
    "id": "OCW8a1ZNupNR",
    "outputId": "9645ee38-8a12-45c1-c033-ff294da44c36"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 200     # Based on ressource limiations\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 32\n",
    "IMG_CH = 3\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "NUM_STEPS = 400\n",
    "\n",
    "# Define paths\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(Path(\".\").resolve()))\n",
    "OUT_DIR = Path(\"generated_flowers\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. MODEL SETUP ---\n",
    "\n",
    "model = UNet_utils.UNet(\n",
    "    T=400, img_ch=3, img_size=32, down_chs=(256, 256, 512),\n",
    "    t_embed_dim=8, c_embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "# Ensure the path is correct relative to your environment\n",
    "weights_path = '/content/drive/MyDrive/Hands-on-CV3/flowerDiff.pth'\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 1. Define Test Prompts\n",
    "test_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\"\n",
    "]\n",
    "\n",
    "print(\"Generating 3 test images with Guidance Scale w=4.0...\")\n",
    "\n",
    "# 2. Generate (Manually calling the sampler to be sure)\n",
    "#    Note: We explicitly pass w=4.0 here.\n",
    "text_tokens = clip.tokenize(test_prompts).to(device)\n",
    "c = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "B = torch.linspace(B_start, B_end, NUM_STEPS).to(device)\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "\n",
    "def _to_01(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.min() < 0:\n",
    "        x = (x + 1) / 2.0\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "\n",
    "# Sample\n",
    "x_test, _ = ddpm_utils.sample_w(\n",
    "    model,\n",
    "    ddpm,\n",
    "    INPUT_SIZE,\n",
    "    NUM_STEPS,\n",
    "    c,\n",
    "    device,\n",
    "    w_tests=[1] #\n",
    ")\n",
    "# 3. Visualize Results\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(len(test_prompts)):\n",
    "    # Convert from [C, H, W] to [H, W, C] for plotting\n",
    "    img_tensor = _to_01(x_test[i]).cpu()\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(f\"Prompt: {test_prompts[i]}\\n(Guidance w=4.0)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MniDa7z8IBy5",
    "outputId": "7238c36a-622a-470f-f241-895957bd2b2a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# --- 1. CONFIGURATION\n",
    "N = 200     # Based on ressource limiations\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 32\n",
    "IMG_CH = 3\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "NUM_STEPS = 400\n",
    "\n",
    "# Define paths\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(Path(\".\").resolve()))\n",
    "OUT_DIR = Path(\"generated_flowers\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. MODEL SETUP ---\n",
    "\n",
    "model = UNet_utils.UNet(\n",
    "    T=400, img_ch=3, img_size=32, down_chs=(256, 256, 512),\n",
    "    t_embed_dim=8, c_embed_dim=512\n",
    ").to(device)\n",
    "\n",
    "# Ensure the path is correct relative to your environment\n",
    "weights_path = '/content/drive/MyDrive/Hands-on-CV3/flowerDiff.pth'\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# DDPM Setup\n",
    "B = torch.linspace(B_start, B_end, NUM_STEPS).to(device)\n",
    "ddpm = ddpm_utils.DDPM(B, device)\n",
    "\n",
    "# --- 3. HOOKS FOR EMBEDDINGS ---\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook on down2\n",
    "_ = model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
    "\n",
    "# --- 4. HELPER FUNCTIONS ---\n",
    "def _to_01(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.min() < 0:\n",
    "        x = (x + 1) / 2.0\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "def cycle_prompts(prompts, n):\n",
    "    \"\"\"Cycles through the prompt list to match the requested number of images.\"\"\"\n",
    "    return (prompts * ((n + len(prompts) - 1) // len(prompts)))[:n]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_flowers_with_embeddings(prompt_list):\n",
    "    embeddings_storage.clear()\n",
    "\n",
    "    text_tokens = clip.tokenize(prompt_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    x_gen, _ = ddpm_utils.sample_w(\n",
    "        model,\n",
    "        ddpm,\n",
    "        INPUT_SIZE,\n",
    "        NUM_STEPS,\n",
    "        c,\n",
    "        device,\n",
    "        w_tests=[2]\n",
    "    )\n",
    "\n",
    "    down2 = embeddings_storage[\"down2\"]          # [B, C, H, W]\n",
    "    down2_vec = down2.mean(dim=(2, 3))           # [B, C] Global Average Pooling\n",
    "\n",
    "    x_gen = x_gen[:len(prompt_list)]\n",
    "    down2_vec = down2_vec[:len(prompt_list)]\n",
    "\n",
    "    return x_gen, down2_vec\n",
    "\n",
    "# --- 5. MAIN GENERATION LOOP ---\n",
    "text_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\"\n",
    "]\n",
    "\n",
    "text_prompts_seed = text_prompts\n",
    "all_prompts = cycle_prompts(text_prompts_seed, N)\n",
    "\n",
    "image_paths = []\n",
    "prompt_per_image = []\n",
    "unet_embs = []\n",
    "\n",
    "print(f\"Starting generation of {N} images...\")\n",
    "\n",
    "idx = 0\n",
    "while idx < N:\n",
    "    # Slice the prompts for this batch\n",
    "    batch_prompts = all_prompts[idx : idx + BATCH_SIZE]\n",
    "\n",
    "    # Generate\n",
    "    x_gen, emb_vec = sample_flowers_with_embeddings(batch_prompts)\n",
    "\n",
    "    # Process results\n",
    "    x01 = _to_01(x_gen).cpu()\n",
    "    emb_np = emb_vec.detach().cpu().numpy()\n",
    "\n",
    "    for j in range(len(batch_prompts)):\n",
    "        current_id = idx + j\n",
    "        fp = OUT_DIR / f\"gen_{current_id:06d}.png\"\n",
    "\n",
    "        save_image(x01[j], fp)\n",
    "\n",
    "        image_paths.append(str(fp))\n",
    "        prompt_per_image.append(batch_prompts[j])\n",
    "        unet_embs.append(emb_np[j].astype(np.float32))\n",
    "\n",
    "    idx += len(batch_prompts)\n",
    "    print(f\"Generated {idx}/{N}\")\n",
    "\n",
    "unet_embs_np = np.stack(unet_embs, axis=0)  # [N, C]\n",
    "print(\"Done!\")\n",
    "print(\"Generated:\", len(image_paths), \"Embeddings:\", unet_embs_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkPPrNTISS9"
   },
   "source": [
    "## Part 2: Evaluation with CLIP Score and Frechet Inception Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul_RZsCIbcHF"
   },
   "source": [
    "First download the data for comparson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1inW9XQNbbKk"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# 1. Define URL and Paths\n",
    "dataset_url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "archive_path = Path(\"flower_photos.tgz\")\n",
    "data_dir = Path(\"flower_photos\")  # This is where images will be extracted\n",
    "\n",
    "# 2. Download\n",
    "if not archive_path.exists():\n",
    "    print(f\"Downloading TF-Flowers from {dataset_url}...\")\n",
    "    urllib.request.urlretrieve(dataset_url, archive_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# 3. Extract\n",
    "if not data_dir.exists():\n",
    "    print(\"Extracting images...\")\n",
    "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "    print(f\"Extracted to {data_dir.resolve()}\")\n",
    "else:\n",
    "    print(f\"Data already exists at {data_dir.resolve()}\")\n",
    "\n",
    "# 4. Quick verification\n",
    "jpg_count = len(list(data_dir.glob(\"**/*.jpg\")))\n",
    "print(f\"Found {jpg_count} images in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2q_-4rZbm7g"
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "# real_data_dir = Path(\"flower_data/train\")\n",
    "\n",
    "# NEW (Points to the downloaded TF-Flowers)\n",
    "real_data_dir = Path(\"flower_photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg6CTfrrulea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOlEHxRpIerX"
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. CLIP Score Evaluation\n",
    "# ==========================================\n",
    "print(\"--- Starting CLIP Evaluation ---\")\n",
    "\n",
    "# FIX 1: Load model ONCE outside the loop\n",
    "# Note: 'ViT-B-32' is standard, but check if you need 'ViT-L-14' for better accuracy if VRAM allows.\n",
    "clip_model_name = \"ViT-B-32\"\n",
    "clip_pretrained = \"laion2b_s34b_b79k\"\n",
    "\n",
    "try:\n",
    "    model_clip, _, preprocess_clip = open_clip.create_model_and_transforms(clip_model_name, pretrained=clip_pretrained)\n",
    "    model_clip = model_clip.to(device).eval()\n",
    "    tokenizer = open_clip.get_tokenizer(clip_model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading OpenCLIP: {e}. Make sure open_clip_torch is installed.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_single_clip_score(image_path, text_prompt):\n",
    "    # Load and Preprocess\n",
    "    image = preprocess_clip(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    text = tokenizer([text_prompt]).to(device)\n",
    "\n",
    "    # Encode\n",
    "    image_features = model_clip.encode_image(image)\n",
    "    text_features = model_clip.encode_text(text)\n",
    "\n",
    "    # Normalize\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Dot product\n",
    "    return float((image_features @ text_features.T).item())\n",
    "\n",
    "# FIX 2: Use 'prompt_per_image' (length 200) instead of 'text_prompts' (length 3)\n",
    "# Ensure image_paths and prompt_per_image exist from the previous step\n",
    "if 'prompt_per_image' not in locals():\n",
    "    print(\"Warning: prompt_per_image not found. Using text_prompts (this limits eval to 3 images).\")\n",
    "    prompt_list_to_use = text_prompts\n",
    "else:\n",
    "    prompt_list_to_use = prompt_per_image\n",
    "\n",
    "# Calculate scores\n",
    "clip_scores = []\n",
    "for i, (p, t) in enumerate(zip(image_paths, prompt_list_to_use)):\n",
    "    score = calculate_single_clip_score(p, t)\n",
    "    clip_scores.append(score)\n",
    "    if i % 50 == 0: print(f\"Evaluated {i} images...\")\n",
    "\n",
    "print(\"Mean CLIP Score:\", float(np.mean(clip_scores)))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. FID (Frechet Inception Distance)\n",
    "# ==========================================\n",
    "print(\"\\n--- Starting FID Evaluation ---\")\n",
    "\n",
    "# FIX 3: Add Normalization. Inception expects ImageNet mean/std.\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Inception V3\n",
    "inception = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n",
    "inception.fc = torch.nn.Identity()  # Remove classification layer\n",
    "inception = inception.to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_inception_embeddings(image_paths, batch_size=32):\n",
    "    embs = []\n",
    "    # Batch processing for speed\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "\n",
    "        imgs = []\n",
    "        valid_batch = True\n",
    "        for p in batch_paths:\n",
    "            try:\n",
    "                imgs.append(inception_transform(Image.open(p).convert(\"RGB\")))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {p}: {e}\")\n",
    "                valid_batch = False\n",
    "\n",
    "        if not valid_batch or len(imgs) == 0: continue\n",
    "\n",
    "        x = torch.stack(imgs).to(device)\n",
    "        y = inception(x)  # [B, 2048]\n",
    "        embs.append(y.cpu().numpy())\n",
    "\n",
    "    if len(embs) > 0:\n",
    "        return np.concatenate(embs, axis=0)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "    # Safety check for small N\n",
    "    if len(real_embeddings) == 0 or len(gen_embeddings) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Check for numerical instability\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return float(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA-xu5p5bsDK"
   },
   "outputs": [],
   "source": [
    "# --- Define Paths ---\n",
    "gen_dir = Path(\"generated_flowers\")\n",
    "\n",
    "# Point to the extracted gt F-Flowers folder\n",
    "real_data_dir = Path(\"flower_photos\")\n",
    "\n",
    "# Collect paths (TF-Flowers has subfolders)\n",
    "gen_paths  = sorted([str(p) for p in gen_dir.glob(\"*.png\")])\n",
    "real_paths = sorted([str(p) for p in real_data_dir.glob(\"**/*.jpg\")])\n",
    "\n",
    "# Limit real images to match the number of generated images (fair comparison)\n",
    "# If we generated 200 images, we select the first 200 real images.\n",
    "if len(real_paths) > len(gen_paths):\n",
    "    real_paths = real_paths[:len(gen_paths)]\n",
    "\n",
    "print(f\"Computing embeddings... Real: {len(real_paths)}, Gen: {len(gen_paths)}\")\n",
    "\n",
    "if len(real_paths) > 0 and len(gen_paths) > 0:\n",
    "    real_emb = get_inception_embeddings(real_paths)\n",
    "    gen_emb  = get_inception_embeddings(gen_paths)\n",
    "\n",
    "    fid_value = calculate_fid(real_emb, gen_emb)\n",
    "    print(f\"FID Score: {fid_value:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping FID: Real or Generated image lists are empty. Check your paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9poQ0C7IgQP"
   },
   "source": [
    "## Part 3: Embedding Analysis with FiftyOne Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIziFrKuIjqT"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting FiftyOne Setup ---\")\n",
    "\n",
    "# 1. Clean up previous runs\n",
    "dataset_name = \"generated_flowers_with_embeddings\"\n",
    "if dataset_name in fo.list_datasets():\n",
    "    fo.delete_dataset(dataset_name)\n",
    "\n",
    "dataset = fo.Dataset(name=dataset_name)\n",
    "\n",
    "# 2. Add Samples\n",
    "samples = []\n",
    "\n",
    "for fp, prompt, score, emb in zip(image_paths, prompt_per_image, clip_scores, unet_embs_np):\n",
    "    s = fo.Sample(filepath=fp)\n",
    "\n",
    "    # Store metadata\n",
    "    s[\"prompt\"] = fo.Classification(label=prompt)\n",
    "    s[\"clip_score\"] = float(score)\n",
    "    s[\"unet_embedding\"] = emb.tolist()  # FiftyOne expects lists, not numpy arrays\n",
    "\n",
    "    samples.append(s)\n",
    "\n",
    "dataset.add_samples(samples)\n",
    "dataset.save()\n",
    "print(f\"Created dataset with {len(dataset)} samples.\")\n",
    "\n",
    "# 3. Brain Computations (Uniqueness & Representativeness)\n",
    "print(\"Computing uniqueness...\")\n",
    "fob.compute_uniqueness(dataset)\n",
    "\n",
    "print(\"Computing representativeness...\")\n",
    "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")\n",
    "\n",
    "# 4. Visualization (UMAP)\n",
    "print(\"Computing UMAP visualization...\")\n",
    "# This generates a 2D scatter plot of your embeddings in the App\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"unet_embedding\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"umap_vis\"\n",
    ")\n",
    "\n",
    "# 5. Launch App\n",
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-DMAbqZ3a5H"
   },
   "source": [
    "The results look quite nice. While they are are a bit noisy, it is clear that using the guidance scale, every image is clearly dividable in one of the 3 possible classes!ðŸŒ¹ðŸŒ»ðŸŒ¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMdaoFqmMajK"
   },
   "outputs": [],
   "source": [
    "## Part 4 - W&B logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cXHDBUusDq1"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "print(\"--- Starting WandB Logging ---\")\n",
    "\n",
    "# 1. Login\n",
    "wandb.login()\n",
    "\n",
    "# 2. Initialize Run\n",
    "run = wandb.init(\n",
    "    project=\"Hands-on-CV-Project3\",\n",
    "    name=\"flower_generation_run\",\n",
    "    config={\n",
    "        \"num_steps\": NUM_STEPS,\n",
    "        \"beta_start\": B_start,\n",
    "        \"beta_end\": B_end,\n",
    "        \"num_prompts\": len(text_prompts),\n",
    "        \"total_images\": N,\n",
    "        \"model_architecture\": \"UNet_32x32\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# 3. Log Scalar Metrics (Summary)\n",
    "run.log({\n",
    "    \"global_clip_mean\": float(np.mean(clip_scores)),\n",
    "    \"global_fid_score\": fid_value,\n",
    "})\n",
    "\n",
    "# 4. Create Rich Table\n",
    "# We iterate over the FiftyOne dataset to ensure we get the computed scores\n",
    "table = wandb.Table(columns=[\n",
    "    \"generated_image\",\n",
    "    \"prompt\",\n",
    "    \"clip_score\",\n",
    "    \"uniqueness_score\",\n",
    "    \"representativeness_score\"\n",
    "])\n",
    "\n",
    "print(\"Populating WandB Table...\")\n",
    "\n",
    "for s in dataset:\n",
    "    # Safely get brain scores (default to 0.0 if calculation failed)\n",
    "    u_score = s[\"uniqueness\"] if \"uniqueness\" in s else 0.0\n",
    "    r_score = s[\"representativeness\"] if \"representativeness\" in s else 0.0\n",
    "\n",
    "    table.add_data(\n",
    "        wandb.Image(s.filepath),\n",
    "        s[\"prompt\"].label,\n",
    "        s[\"clip_score\"],\n",
    "        u_score,\n",
    "        r_score\n",
    "    )\n",
    "\n",
    "# 5. Log Table and Finish\n",
    "run.log({\"generation_results\": table})\n",
    "run.finish()\n",
    "\n",
    "print(\"WandB logging complete ðŸš€ðŸš€ðŸš€ Check your dashboard!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8hReWM56RCo"
   },
   "source": [
    "Publish the data on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sAohqqt6QYi"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "import fiftyone as fo\n",
    "from huggingface_hub import login\n",
    "# --- SETUP ---\n",
    "my_hf_REDACTED = \"hf_REDACTED\" # PUT YOUR TOKEN HERE\n",
    "login(token=my_hf_REDACTED)\n",
    "\n",
    "# --- LOAD DATASET ---\n",
    "print(\"1. Attempting to load dataset...\", flush=True)\n",
    "\n",
    "# Ensure we aren't using a cached variable by accident\n",
    "if 'dataset' in locals():\n",
    "    del dataset\n",
    "\n",
    "dataset = fo.load_dataset(\"generated_flowers_with_embeddings\")\n",
    "print(\"   Dataset loaded successfully.\", flush=True)\n",
    "\n",
    "# --- UPLOAD ---\n",
    "hf_REDACTED_name = \"Consscht/FlowerDiff\"\n",
    "\n",
    "print(f\"2. Preparing to upload to: {hf_REDACTED_name}...\", flush=True)\n",
    "print(\"   (This may take a moment while FiftyOne prepares the files...)\", flush=True)\n",
    "\n",
    "dataset.push_to_hub(\n",
    "    repo_id=hf_REDACTED_name,\n",
    "    private=False,\n",
    "    dataset_type=\"image\"\n",
    ")\n",
    "\n",
    "print(\"3. Success! Your dataset is now published ðŸš€\", flush=True)\n",
    "print(f\"View it here: https://huggingface.co/datasets/{hf_REDACTED_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65N41PeuDFp5"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "import fiftyone as fo\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# !pip install huggingface_hub  # Uncomment if you need to install it\n",
    "\n",
    "import fiftyone as fo\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "# Paste your token starting with \"hf_...\" inside the quotes below\n",
    "my_hf_REDACTED = \"hf_REDACTED\"\n",
    "\n",
    "# Log in automatically (no pop-up box)\n",
    "login(token=my_hf_REDACTED)\n",
    "\n",
    "# --- 2. LOAD DATASET ---\n",
    "# Reloads the dataset you created in the previous step\n",
    "if 'dataset' not in locals():\n",
    "    dataset = fo.load_dataset(\"generated_flowers_with_embeddings\")\n",
    "\n",
    "# --- 3. UPLOAD TO HUGGING FACE ---\n",
    "# Defines your repository name (Username/DatasetName)\n",
    "# Based on your previous attempt, I assume your username is 'Consscht'\n",
    "hf_REDACTED_name = \"Consscht/FlowerDiff\"\n",
    "\n",
    "print(f\"ðŸš€ Uploading dataset to: {hf_REDACTED_name} ...\")\n",
    "\n",
    "dataset.push_to_hub(\n",
    "    repo_id=hf_REDACTED_name,\n",
    "    private=False,       # Set to True if you want it hidden\n",
    "    dataset_type=\"image\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Success! Your dataset is now published.\")\n",
    "print(f\"View it here: https://huggingface.co/datasets/{hf_REDACTED_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIQ-lKivsOT-"
   },
   "source": [
    "## Bonus -- \"MNIST\" idk classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2fijGTQv_Nw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "\n",
    "# Import utils (ensure utils.py is in the same directory)\n",
    "import utils\n",
    "from utils import ClassicLeNet5 \n",
    "\n",
    "# Note: UNet_utils and ddpm_utils must be available from earlier validation cells\n",
    "# If they are not in the path, ensure the corresponding files exist or cells are run.\n",
    "# from utils import UNet_utils, ddpm_utils  <-- These are not in utils.py\n",
    "\n",
    "# --- 1. SETUP & CONFIG ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "THRESHOLD = 0.8         # (Optional) Confidence threshold if we wanted to enforce high confidence\n",
    "N_SAMPLES = 50          # How many images to generate\n",
    "OUT_DIR = Path(\"generated_mnist_bonus\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 2. LOAD MODELS ---\n",
    "\n",
    "# A. Load the Trained IDK Classifier\n",
    "# We use the 11-class model we trained in IDK_Model_Training.ipynb\n",
    "classifier = ClassicLeNet5(num_classes=11)\n",
    "classifier_path = \"best_lenet_idk.pth\"\n",
    "\n",
    "if Path(classifier_path).exists():\n",
    "    classifier.load_state_dict(torch.load(classifier_path, map_location=device))\n",
    "    print(f\"Loaded IDK Classifier from {classifier_path}\")\n",
    "else:\n",
    "    print(f\"WARNING: Classifier checkpoint '{classifier_path}' not found!\")\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "\n",
    "# B. Load the MNIST Generator (U-Net)\n",
    "# IMPORTANT: Use img_ch=1 and img_size=28 for MNIST!\n",
    "# Assuming UNet_utils is defined in the workspace or imported previously\n",
    "try:\n",
    "    if 'UNet_utils' in globals():\n",
    "       unet_class = UNet_utils.UNet\n",
    "    else:\n",
    "       # Fallback: hope it's in the scope or accessible\n",
    "       import UNet_utils\n",
    "       unet_class = UNet_utils.UNet\n",
    "       \n",
    "    model = unet_class(\n",
    "        T=400, img_ch=1, img_size=28, down_chs=(256, 256, 512),\n",
    "        t_embed_dim=8, c_embed_dim=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Check for DDPM Utils\n",
    "    if 'ddpm_utils' in globals():\n",
    "        ddpm_class = ddpm_utils.DDPM\n",
    "    else:\n",
    "        import ddpm_utils\n",
    "        ddpm_class = ddpm_utils.DDPM\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Warning: UNet_utils or ddpm_utils not found. Ensure they are imported or defined.\")\n",
    "    # Stop execution if important utils are missing\n",
    "    # raise\n",
    "\n",
    "# For this bonus part, you likely need a different checkpoint for the generator (trained on MNIST)\n",
    "# If you don't have one, this part might just generate noise.\n",
    "# model.load_state_dict(torch.load('my_mnist_generator.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Setup DDPM for generation\n",
    "B = torch.linspace(0.0001, 0.02, 400).to(device)\n",
    "try:\n",
    "    ddpm = ddpm_class(B, device)\n",
    "except:\n",
    "    print(\"Could not initialize DDPM. Skipping generation setup.\")\n",
    "\n",
    "# --- 3. DEFINE PREDICTION FUNCTION ---\n",
    "def predict_with_idk(image, model, threshold):\n",
    "    \"\"\"\n",
    "    Predicts using the 11-class model.\n",
    "    Index 0-9: Digits\n",
    "    Index 10: IDK\n",
    "    \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        logits = model(image)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        max_prob, pred_idx = torch.max(probs, dim=1)\n",
    "        \n",
    "        idx = pred_idx.item()\n",
    "        \n",
    "        # If model explicitly predicts the IDK class (Index 10)\n",
    "        if idx == 10:\n",
    "            return \"IDK\", max_prob.item()\n",
    "        \n",
    "        # Optional: You could still force IDK if confidence of a digit is too low\n",
    "        # if max_prob.item() < threshold:\n",
    "        #     return \"IDK\", max_prob.item()\n",
    "            \n",
    "        return str(idx), max_prob.item()\n",
    "\n",
    "# --- 4. GENERATE, CLASSIFY & BUILD DATASET ---\n",
    "\n",
    "# Initialize FiftyOne Dataset\n",
    "dataset_name = \"mnist_idk_experiment\"\n",
    "if dataset_name in fo.list_datasets():\n",
    "    fo.delete_dataset(dataset_name)\n",
    "dataset = fo.Dataset(name=dataset_name)\n",
    "\n",
    "print(f\"Generating {N_SAMPLES} digits and classifying...\")\n",
    "\n",
    "samples = []\n",
    "\n",
    "# Ensure we can run generation\n",
    "if 'ddpm' in locals() and 'model' in locals():\n",
    "    for i in range(N_SAMPLES):\n",
    "        # A. Generate Image\n",
    "        # Start with random noise [1, 1, 28, 28]\n",
    "        xi = torch.randn(1, 1, 28, 28).to(device)\n",
    "        \n",
    "        try:\n",
    "             x_gen, _ = ddpm_utils.sample_w(model, ddpm, (1, 28, 28), 400, xi, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Generation failed: {e}\")\n",
    "            break\n",
    "\n",
    "        # B. Classify\n",
    "        # x_gen is [-1, 1], ClassicLeNet5 expects normalized but close to [0,1] or standard normalization\n",
    "        # Our LeNet was trained on images normalized with mean=0.1307, std=0.3081\n",
    "        # Generator output [-1, 1].\n",
    "        # First map to [0, 1]\n",
    "        img_01 = (x_gen.clamp(-1, 1) + 1) / 2.0\n",
    "        \n",
    "        # Then normalize for LeNet\n",
    "        # (x - mean) / std\n",
    "        img_norm = (img_01 - 0.1307) / 0.3081\n",
    "        \n",
    "        label, confidence = predict_with_idk(img_norm, classifier, THRESHOLD)\n",
    "\n",
    "        # C. Save Image to Disk (required for FiftyOne)\n",
    "        # Save the [0,1] version for viewing\n",
    "        fp = OUT_DIR / f\"mnist_{i:04d}.png\"\n",
    "        save_image(img_01, fp)\n",
    "\n",
    "        # D. Create FiftyOne Sample\n",
    "        sample = fo.Sample(filepath=str(fp))\n",
    "\n",
    "        # Store the prediction\n",
    "        sample[\"prediction\"] = fo.Classification(\n",
    "            label=label,\n",
    "            confidence=confidence\n",
    "        )\n",
    "\n",
    "        # Tag it based on result\n",
    "        if label == \"IDK\":\n",
    "            sample.tags.append(\"idk_predicted\")\n",
    "        else:\n",
    "            sample.tags.append(\"digit_predicted\")\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Add samples to dataset\n",
    "    if samples:\n",
    "        dataset.add_samples(samples)\n",
    "        dataset.save()\n",
    "        print(f\"Done! Created dataset '{dataset_name}' with {len(samples)} samples.\")\n",
    "        \n",
    "        # --- 5. VISUALIZE ---\n",
    "        # This opens the App inside the notebook\n",
    "        session = fo.launch_app(dataset)\n",
    "else:\n",
    "    print(\"Skipping generation loop because model/ddpm not initialized.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOWWCDlFhP5ZutyzmVTRqV3",
   "gpuType": "T4",
   "mount_file_id": "1c0-Ahw6cQAmC8TwUy9hxz3PxVToRvlm8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
